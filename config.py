CONFIG = {
    "vocab_size": 50257,  # GPT-2 vocab size
    "max_seq_len": 128,  # Maximum sequence length
    "embedding_dim": 768,  # Hidden size of embeddings
    "num_heads": 12,  # Number of attention heads
    "num_layers": 12,  # Number of transformer layers
    "hidden_dim": 3072,  # Feedforward network dimension
    "batch_size": 8,  # Batch size
    "learning_rate": 5e-5,  # Learning rate
    "epochs": 1000,  # Number of epochs
}
